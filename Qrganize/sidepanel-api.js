// Qrganize/sidepanel-api.js
import { stripThink, esc as escapeHTML, decodeHtmlEntities } from "./sidepanel-utils.js";
import { getConfig, getLevelText } from "./sidepanel-config.js"; // Removed getChatUrl
import { S } from "./sidepanel-state.js";

async function makeApiRequest(url, method, headers, body, signal, providerName, timeoutMs, showErrDetail) {
    const controller = new AbortController();
    let timeoutId = null;

    // Use the provided signal directly if it's the only one, otherwise combine.
    // The external signal (userSignal from fetchAI) should be able to abort the request.
    // The internal timeout controller also needs to be able to abort.
    // AbortSignal.any requires Node.js 20+ or specific browser versions.
    // For broader compatibility, we'll manage combined signals manually if AbortSignal.any is not available or suitable.
    // However, the original code used a simpler approach by just having the userSignal abort the controller.
    // And the controller's signal was used for fetch. Let's stick to that pattern for now.

    const fetchSignal = controller.signal; // This signal will be used for the fetch request.

    if (timeoutMs > 0) {
        timeoutId = setTimeout(() => {
            // console.debug(`[API] Timeout for ${providerName} after ${timeoutMs}ms`);
            controller.abort(new DOMException('TimeoutError', `TimeoutError (${timeoutMs / 1000}s)`));
        }, timeoutMs);
    }
    
    const handleExternalAbort = () => {
        // console.debug(`[API] External signal aborted for ${providerName}`);
        if (timeoutId) clearTimeout(timeoutId);
        controller.abort(signal.reason || new DOMException('AbortError', 'ExternalAbort'));
    };

    if (signal) { // userSignal from fetchAI
        if (signal.aborted) {
            if (timeoutId) clearTimeout(timeoutId);
            throw signal.reason || new DOMException('AbortError', 'ExternalAbort (pre-aborted)');
        }
        signal.addEventListener('abort', handleExternalAbort, { once: true });
    }

    // Clean up external listener when fetchSignal aborts (e.g. due to timeout)
    fetchSignal.addEventListener('abort', () => {
        if (timeoutId) clearTimeout(timeoutId);
        if (signal) signal.removeEventListener('abort', handleExternalAbort);
    }, { once: true });


    try {
        const response = await fetch(url, {
            method,
            headers,
            body: body ? JSON.stringify(body) : undefined,
            signal: fetchSignal
        });

        if (timeoutId) clearTimeout(timeoutId);
        if (signal) signal.removeEventListener('abort', handleExternalAbort);


        if (!response.ok) {
            const responseBodyText = await response.text().catch(() => "");
            let errorMsgText;
            // Try to parse JSON error from cloud providers
            let detail = "";
            try {
                const jsonError = JSON.parse(responseBodyText);
                if (jsonError.error && jsonError.error.message) {
                    detail = jsonError.error.message;
                } else if (typeof jsonError === 'string') {
                    detail = jsonError;
                } else {
                    detail = responseBodyText.substring(0, 200) + (responseBodyText.length > 200 ? "..." : "");
                }
            } catch (e) {
                detail = responseBodyText.substring(0, 200) + (responseBodyText.length > 200 ? "..." : "");
            }
            
            if (showErrDetail) {
                errorMsgText = `API Error (${providerName}, HTTP ${response.status} ${response.statusText}): ${detail}`;
            } else {
                errorMsgText = `AI ä¼ºæœå™¨è«‹æ±‚å¤±æ•— (${providerName}, HTTP ${response.status} ${response.statusText})ã€‚é–‹å•Ÿè¨­å®šä¸­çš„ã€Œé¡¯ç¤ºè©³ç´°éŒ¯èª¤è¨Šæ¯ã€ä»¥ç²å–æ›´å¤šè³‡è¨Šã€‚`;
            }
            throw new Error(errorMsgText);
        }
        // For LM Studio, response might be HTML escaped JSON
        const rawResponseText = await response.text();
        if (providerName === "lmstudio" && rawResponseText.includes("&quot;")) {
            const decodedText = decodeHtmlEntities(rawResponseText);
            return JSON.parse(decodedText);
        }
        return JSON.parse(rawResponseText);

    } catch (error) {
        if (timeoutId) clearTimeout(timeoutId);
        if (signal) signal.removeEventListener('abort', handleExternalAbort);

        if (error.name === 'AbortError' || error.name === 'TimeoutError') {
             // Check if it was the external signal or timeout
            if (signal && signal.aborted) {
                 // console.debug(`[API] Request aborted by external signal for ${providerName}:`, signal.reason);
                 throw signal.reason; // Re-throw the original abort reason
            } else {
                 // console.debug(`[API] Request timed out for ${providerName}`);
                 throw new Error(`è«‹æ±‚ ${providerName} API è¶…æ™‚ (è¶…é ${timeoutMs / 1000} ç§’)`);
            }
        }
        // Re-throw other errors (including custom ones from !response.ok)
        throw error;
    }
}


export async function fetchAI(promptContent, userSignal = null) {
    const config = getConfig();
    const { apiProvider, model, aiTimeout, apiUrl, showErr } = config;

    let url = "";
    let headers = { "Content-Type": "application/json" };
    let requestBody = {};
    let responseExtractor;

    // Default system prompt for OpenAI-like APIs (optional)
    const systemPrompt = "You are a helpful assistant specializing in text summarization and question answering based on provided content.";


    switch (apiProvider) {
        case "ollama":
            url = `${apiUrl.replace(/\/+$/, '')}/api/chat`;
            // Ollama's /api/chat expects a messages array.
            // If promptContent is just the user's text, wrap it.
            // The old Ollama payload was: { model: cfg.model, messages: [ { role: "user", content: promptText } ], stream: false };
            // The very old one was: { model: cfg.model, prompt: promptContent, stream: false };
            // The /api/generate endpoint uses "prompt", but /api/chat uses "messages".
            // Let's assume promptContent is the full user message here.
            requestBody = { 
                model: model, 
                messages: [{ role: "user", content: promptContent }], 
                stream: false 
            };
            responseExtractor = data => {
                if (data && data.message && typeof data.message.content === 'string') {
                    return data.message.content;
                }
                throw new Error("Invalid Ollama response structure: 'message.content' missing or not a string.");
            };
            break;
        case "lmstudio":
            url = `${apiUrl.replace(/\/+$/, '')}/v1/chat/completions`;
            requestBody = { 
                model: model, // LM Studio uses this to identify the loaded model
                messages: [
                    { role: "system", content: systemPrompt },
                    { role: "user", content: promptContent }
                ], 
                stream: false 
            };
            responseExtractor = data => {
                if (data && data.choices && data.choices[0] && data.choices[0].message && typeof data.choices[0].message.content === 'string') {
                    return data.choices[0].message.content;
                }
                throw new Error("Invalid LM Studio response structure: 'choices[0].message.content' missing or not a string.");
            };
            break;
        case "chatgpt":
            url = "https://api.openai.com/v1/chat/completions";
            if (!config.chatgptApiKey) throw new Error("ChatGPT API key is missing.");
            headers["Authorization"] = `Bearer ${config.chatgptApiKey}`;
            requestBody = { 
                model: model, 
                messages: [
                    { role: "system", content: systemPrompt },
                    { role: "user", content: promptContent }
                ], 
                stream: false 
            };
            responseExtractor = data => data.choices[0].message.content;
            break;
        case "groq":
            url = "https://api.groq.com/openai/v1/chat/completions";
            if (!config.groqApiKey) throw new Error("Groq API key is missing.");
            headers["Authorization"] = `Bearer ${config.groqApiKey}`;
            requestBody = { 
                model: model, 
                messages: [
                    { role: "system", content: systemPrompt },
                    { role: "user", content: promptContent }
                ], 
                stream: false 
            };
            responseExtractor = data => data.choices[0].message.content;
            break;
        case "gemini":
            if (!config.geminiApiKey) throw new Error("Gemini API key is missing.");
            if (!model) throw new Error("Gemini model name is missing (e.g., gemini-pro).");
            url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${config.geminiApiKey}`;
            requestBody = {
                contents: [{ parts: [{ text: promptContent }] }],
                generationConfig: {
                    // temperature: 0.7, // Example
                }
            };
            responseExtractor = data => {
                if (data.candidates && data.candidates[0] && data.candidates[0].content && data.candidates[0].content.parts && data.candidates[0].content.parts[0]) {
                    return data.candidates[0].content.parts[0].text;
                }
                if (data.promptFeedback && data.promptFeedback.blockReason) {
                    const safetyRatings = data.promptFeedback.safetyRatings ? JSON.stringify(data.promptFeedback.safetyRatings) : "No details";
                    throw new Error(`Content blocked by Gemini due to: ${data.promptFeedback.blockReason}. Ratings: ${safetyRatings}`);
                }
                throw new Error("Invalid Gemini response structure or content blocked without explicit reason.");
            };
            break;
        case "deepseek":
            url = "https://api.deepseek.com/chat/completions";
            if (!config.deepseekApiKey) throw new Error("DeepSeek API key is missing.");
            headers["Authorization"] = `Bearer ${config.deepseekApiKey}`;
            requestBody = { 
                model: model, 
                messages: [
                    { role: "system", content: systemPrompt },
                    { role: "user", content: promptContent }
                ], 
                stream: false 
            };
            responseExtractor = data => data.choices[0].message.content;
            break;
        default:
            throw new Error(`Unsupported API provider: ${apiProvider}`);
    }

    try {
        const rawJsonResponseFromAPI = await makeApiRequest(url, "POST", headers, requestBody, userSignal, apiProvider, aiTimeout * 1000, showErr);
        const extractedContent = responseExtractor(rawJsonResponseFromAPI);

        if (config.directOutput) {
            return extractedContent; // Always return plain text if directOutput is true
        } else {
            // If directOutput is false, we want structured JSON if possible.
            // For Ollama and LMStudio, if they were prompted for JSON, their 'extractedContent' IS the JSON string.
            // For cloud APIs, their 'extractedContent' is typically plain text unless specifically prompted for JSON string output.
            // The current prompts for cloud APIs are for plain text summary.
            // So, for cloud APIs or if the extracted content is not meant to be a JSON string, we return the extracted text.
            if (apiProvider === 'ollama' || apiProvider === 'lmstudio') {
                // Assuming that if not in directOutput mode, these providers were prompted to return a JSON string.
                // The responseExtractor for these should yield that JSON string.
                return extractedContent; // This IS the JSON string to be parsed by sidepanel-main.
            } else {
                // For cloud APIs, even if directOutput is false, we assume the prompt was for a good textual summary,
                // not a specific JSON structure that parseAIJsonResponse would handle.
                // Thus, we return the plain text content.
                return extractedContent;
            }
        }
    } catch (error) {
        // console.error(`[API] Error in fetchAI for ${apiProvider}:`, error);
        // Ensure the error is re-thrown to be caught by the caller (e.g., summarizeContent, askAIQuestion)
        throw error; 
    }
}


// buildSummaryPrompt å‡½å¼ (å·²ä¿®æ”¹)
export function buildSummaryPrompt(title, content, isDirectOutputModeOverride) {
    const cfg = getConfig();
    const levelText = getLevelText();
    // Determine the mode: use override if provided, otherwise use config setting
    const actualIsDirectOutputMode = typeof isDirectOutputModeOverride === 'boolean' ? isDirectOutputModeOverride : cfg.directOutput;
    let prompt;

    // Note: The promptContent generated here might need to be tailored if results 
    // are poor for specific providers (e.g., some models might benefit from different system prompts or formatting).
    // The current structure is generic.

    if (actualIsDirectOutputMode) {
        prompt = `æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„å…§å®¹åˆ†æå¸«ã€‚è«‹å°‡ä»¥ä¸‹æä¾›çš„ã€ŒåŸå§‹å…§å®¹ã€æ•´ç†æˆä¸€æ®µæˆ–æ•¸æ®µæµæš¢æ˜“è®€çš„æ‘˜è¦ã€‚

é‡è¦æŒ‡ç¤ºï¼š
- è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸è¦ä½¿ç”¨ä»»ä½• JSON æ ¼å¼æˆ–ä»»ä½•ç¨‹å¼ç¢¼å€å¡Šã€‚
- æ‘˜è¦æ‡‰å®Œå…¨åŸºæ–¼ã€ŒåŸå§‹å…§å®¹ã€ï¼Œä¸å¾—æ¨è«–æˆ–å»¶ä¼¸åŸæ–‡æœªæåŠçš„è³‡è¨Šã€‚
- æ‘˜è¦çš„èªè¨€è«‹ä½¿ç”¨ï¼šã€Œ${cfg.outputLanguage}ã€ã€‚
- æ‘˜è¦çš„è©³ç´°ç¨‹åº¦è«‹åƒè€ƒï¼šã€Œ${levelText}ã€ã€‚
- å¦‚æœã€ŒåŸå§‹å…§å®¹ã€éæ–¼ç°¡çŸ­æˆ–ä¸é©åˆç”Ÿæˆæ‘˜è¦ï¼Œè«‹ç›´æ¥å›ç­”ã€Œå…§å®¹éæ–¼ç°¡çŸ­ï¼Œç„¡æ³•æä¾›æœ‰æ„ç¾©çš„æ‘˜è¦ã€‚ã€

æ‘˜è¦æ’°å¯«æº–å‰‡ï¼š
- è«‹å…ˆåˆ¤æ–·åŸå§‹å…§å®¹å±¬æ–¼å“ªä¸€é¡å‹ï¼Œå†ä¾è©²é¡å‹çš„è®€è€…æœ€é—œå¿ƒçš„é‡é»é€²è¡Œæ‘˜è¦ã€‚
- æ‰€æœ‰è³‡è¨Šè«‹ä¾æ“šã€Œå°ä¸€èˆ¬ä½¿ç”¨è€…çš„é‡è¦æ€§ã€ç”±é«˜è‡³ä½æ’åˆ—ï¼Œå…ˆèªªæ˜æœ€æœ‰åƒ¹å€¼çš„é‡é»ã€‚
- è‹¥ç‚º**ä¸€èˆ¬æ–‡ç« é¡å‹**ï¼ˆå¦‚æ•£æ–‡ã€å¿ƒå¾—ã€è§€é»é™³è¿°ç­‰ï¼‰ï¼Œè«‹æ”¹æ¡ã€Œæ®µè½æ‘˜è¦ã€çš„æ–¹å¼ï¼Œé€æ®µæç…‰å‡ºé‡é»ï¼Œæœ€å¾Œè£œä¸Šä¸€æ®µç°¡æ½”æ˜ç¢ºçš„ã€Œç¸½çµã€ï¼Œæ­¸ç´æ•´ç¯‡å…§å®¹çš„æ ¸å¿ƒæ€æƒ³æˆ–ç›®çš„ã€‚
- ä»¥ä¸‹é¡å‹åˆ†é¡åƒ…ä¾›æ‚¨åƒè€ƒåˆ¤æ–·æ–‡ç« å±¬æ€§åŠæ“·å–é‡é»çš„æ–¹å‘ï¼š
  - æ–°èå ±å°ï¼åˆ†ææ–‡ç«  â†’ äº‹ä»¶æ¦‚è¦ã€æ ¸å¿ƒè«–é»ã€é—œéµæ•¸æ“šã€è§€é»ä¾†æºã€çµè«–æ‘˜è¦
  - ç”¢å“æˆ–æœå‹™ä»‹ç´¹ â†’ åç¨±ã€åŠŸèƒ½ç‰¹è‰²ã€ä½¿ç”¨æ•ˆç›Šã€åƒ¹æ ¼è³‡è¨Šã€ç”¨æˆ¶è©•åƒ¹
  - åœ°é»è©•è«–ï¼ç¾é£Ÿä»‹ç´¹ â†’ åœ°å€æˆ–åœ°é»åç¨±ã€äº¤é€šã€ç’°å¢ƒæ°›åœã€åƒ¹ä½ã€ç‰¹è‰²æ¨è–¦
  - æ•™å­¸ï¼æŒ‡å—ï¼é£Ÿè­œ â†’ æˆå“ç‰¹æ€§ã€æ­¥é©Ÿæµç¨‹ã€æ‰€éœ€ææ–™ã€æ™‚é–“ä¼°è¨ˆã€è¨£ç«…æé†’
  - å¾µæ‰è³‡è¨Š â†’ å…¬å¸è³‡è¨Šã€è·ç¼ºåç¨±ã€è·è²¬èªªæ˜ã€å¾…é‡æ¢ä»¶ã€æ‡‰å¾µè³‡æ ¼
  - è«–å£‡è¨è«–ï¼å•ç­”å½™æ•´ â†’ è¨è«–ä¸»é¡Œã€è§€é»å½™æ•´ã€å¸¸è¦‹å»ºè­°ã€å…±è­˜çµè«–

åŸå§‹å…§å®¹å¦‚ä¸‹ï¼š
æ¨™é¡Œï¼š${title}
å…§å®¹ï¼š${preprocessInputForAI(content)}`;

    } else {
        // Existing JSON prompt logic
        prompt = `æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„å…§å®¹åˆ†æå¸«ã€‚è«‹å°‡ä»¥ä¸‹å…§å®¹æ•´ç†æˆæ•¸å€‹ä¸»è¦é‡é»ï¼Œä¸¦ä»¥ã€Œ${cfg.outputLanguage}ã€æ’°å¯«ã€‚

âš ï¸ è«‹**åƒ…è¼¸å‡ºä¸€å€‹åˆæ³•çš„ JSON ç‰©ä»¶**ï¼Œä¸å¾—åŒ…å«ä»»ä½•èªªæ˜ã€‚

JSON ç‰©ä»¶æ ¼å¼å¦‚ä¸‹ï¼š
{
  "keyPoints": [
    {
      "title": "ï¼ˆå­—ä¸²ï¼‰é‡é»çš„ç°¡æ½”æ¨™é¡Œ",
      "details": "ï¼ˆå­—ä¸²ï¼‰è©³ç´°èªªæ˜ï¼Œå¯ç”¨ \\n è¡¨ç¤ºæ®µè½æ›è¡Œ",
      "quote": "ï¼ˆå­—ä¸²ï¼Œå¯é¸ï¼‰å¼•ç”¨åŸæ–‡ä¸­çš„ä¸€å¥è©±ï¼Œå¦‚ç„¡å¯çœç•¥æˆ–ç•™ç©º"
    }
  ]
}

æ ¼å¼è¦ç¯„ï¼š
- åƒ…è¼¸å‡ºä¸Šè¿° JSON ç‰©ä»¶æ ¼å¼ çµæ§‹
- æ­£ç¢ºè½‰ç¾©æ‰€æœ‰ç‰¹æ®Šå­—å…ƒï¼š
  - " â†’ \\"
  - \\ â†’ \\\\
  - å¯¦é«”æ›è¡Œç¬¦ â†’ \\n
- ç¦æ­¢æœªè½‰ç¾©çš„æ§åˆ¶å­—å…ƒï¼ˆå¦‚å¯¦é«”æ›è¡Œã€tabã€Unicode æ§åˆ¶ç¬¦ï¼‰
- ç¦æ­¢å›å‚³ç©ºçš„ keyPoints é™£åˆ—ï¼Œè‹¥ç„¡æ³•ç”¢å‡ºæœ‰æ•ˆå…§å®¹è«‹ä¾ä¸‹æ–¹æ ¼å¼å›å‚³ï¼š

{
  "keyPoints": [
    {
      "title": "å…§å®¹éç°¡",
      "details": "åŸå§‹å…§å®¹éæ–¼ç°¡çŸ­ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆæ‘˜è¦ã€‚",
      "quote": ""
    }
  ]
}

æ‘˜è¦æ’°å¯«æŒ‡å¼•ï¼š
- è«‹å…ˆåˆ¤æ–·åŸå§‹å…§å®¹å±¬æ–¼å“ªä¸€é¡å‹ï¼Œä¸¦ä¾æ“šè©²é¡å‹ä¸­**ä¸€èˆ¬ä½¿ç”¨è€…æœ€é—œå¿ƒçš„è³‡è¨Š**é€²è¡Œæ‘˜è¦ã€‚
- æ‰€æœ‰é‡é»è«‹ä¾ã€Œå°ä¸€èˆ¬ä½¿ç”¨è€…çš„é‡è¦æ€§ã€ç”±é«˜è‡³ä½æ’åˆ—ã€‚
- é‡é»çš„ã€Œtitleã€æ‡‰ç°¡æ½”æ˜ç¢ºï¼Œæ¸…æ¥šå‚³é”è©²é»ä¸»æ—¨ã€‚
- ã€Œdetailsã€è«‹ä»¥æ¸…æ¥šé‚è¼¯æ’°å¯«ï¼Œå…·é«”èªªæ˜èƒŒæ™¯ã€å½±éŸ¿æˆ–ä½¿ç”¨æƒ…å¢ƒï¼Œå¿…è¦æ™‚å¯åˆ†æ®µã€‚
- "quote" æ¬„ä½å¯é¸ï¼Œå¦‚æœ‰åˆé©å¼•ç”¨è«‹æä¾›ï¼Œå¦å‰‡è«‹çœç•¥è©²æ¬„æˆ–è¨­ç‚ºç©ºå­—ä¸²ã€‚

ğŸ“Œ è‹¥åˆ¤æ–·åŸå§‹å…§å®¹å±¬æ–¼**ä¸€èˆ¬æ–‡ç« é¡å‹**ï¼ˆå¦‚æ•£æ–‡ã€å¿ƒå¾—ã€è§€é»é™³è¿°ç­‰ï¼‰ï¼Œè«‹ä¾æ“šåŸæ–‡æ®µè½é †åºæ‘˜è¦æ¯æ®µè¦é»ï¼Œæœ€å¾Œè£œä¸Šä¸€å‰‡ã€Œç¸½çµã€é‡é»ï¼Œæ­¸ç´æ•´é«”æ ¸å¿ƒä¸»æ—¨æˆ–ç«‹æ„ã€‚

ğŸ“š ä»¥ä¸‹ç‚ºä¸åŒé¡å‹å¯èƒ½çš„é‡é»åƒè€ƒæ–¹å‘ï¼Œ**åƒ…ä¾›åˆ¤æ–·æ™‚åƒè€ƒä½¿ç”¨ï¼Œä¸ä»£è¡¨å¼·åˆ¶åˆ†é¡**ï¼š
- æ–°èï¼åˆ†ææ–‡ï¼šäº‹ä»¶æ¦‚è¦ã€æ ¸å¿ƒè«–é»ã€é—œéµæ•¸æ“šã€è§€é»ä¾†æºã€çµè«–æ‘˜è¦
- ç”¢å“ï¼æœå‹™ä»‹ç´¹ï¼šåç¨±ã€åŠŸèƒ½ç‰¹è‰²ã€ä¸»è¦è¦æ ¼ã€åƒ¹æ ¼æ–¹å¼ã€ç”¨æˆ¶å›é¥‹
- åœ°é»ï¼ç¾é£Ÿä»‹ç´¹ï¼šåœ°å€æˆ–åœ°é»åç¨±ã€äº¤é€šè³‡è¨Šã€ç’°å¢ƒæ°›åœã€äººå‡æ¶ˆè²»ã€æ¨è–¦äº®é»
- æ•™å­¸ï¼æŒ‡å—ï¼é£Ÿè­œï¼šå­¸ç¿’ç›®æ¨™ã€æ­¥é©Ÿæµç¨‹ã€ææ–™å·¥å…·ã€æ™‚é–“ä¼°è¨ˆã€è¨£ç«…æç¤º
- å¾µæ‰è³‡è¨Šï¼šå…¬å¸ä»‹ç´¹ã€è·ç¼ºåç¨±ã€å·¥ä½œè·è²¬ã€å¾…é‡æ¢ä»¶ã€æ‡‰å¾µè³‡æ ¼
- è«–å£‡ï¼å•ç­”å½™æ•´ï¼šè¨è«–ä¸»é¡Œã€ä¸»æµè§€é»ã€å…±è­˜å»ºè­°ã€ç†±é–€å›è¦†æ‘˜è¦

â—è«‹åƒ…ä¾æ“šåŸæ–‡æ’°å¯«ï¼Œä¸å¾—æ¨è«–æˆ–å»¶ä¼¸æœªæåŠè³‡è¨Šã€‚

æ‘˜è¦è©³ç•¥ç¨‹åº¦è«‹åƒè€ƒï¼šã€Œ${levelText}ã€

åŸå§‹å…§å®¹å¦‚ä¸‹ï¼š
æ¨™é¡Œï¼š${title}
å…§å®¹ï¼š${preprocessInputForAI(content)}`;

    }
    
    return prompt;
}

function preprocessInputForAI(rawText) {
    if (typeof rawText !== 'string') return '';

    let result = rawText;

    // æ¸…é™¤å¸¸è¦‹ç„¡æ•ˆåˆ†éš”ç¬¦æˆ–è£é£¾ç¬¦è™Ÿ
    result = result.replace(/[â˜…â˜†â—â–²â€»â—†â– â–â—â—â—†â–º]|={3,}|-{3,}|\*{3,}|\/{2,}|\.{3,}/g, '');

    // æ›¿æ› markdownã€ç¶²é å¸¸è¦‹ç¬¦è™Ÿ
    result = result
        .replace(/^#+\s*/gm, '') // # æ¨™é¡Œç§»é™¤
        .replace(/^\s*[\-\*+]\s+/gm, '') // ç§»é™¤åˆ—é»ç¬¦è™Ÿ
        .replace(/\r\n|\r/g, '\n') // çµ±ä¸€æ›è¡Œç¬¦è™Ÿ

    // ç§»é™¤å¤šé¤˜ç©ºè¡Œï¼ˆ3 è¡Œä»¥ä¸Šè¦–ç‚ºå†—é¤˜ï¼‰
    result = result.replace(/\n{3,}/g, '\n\n');

    // å»é™¤ç©ºç™½è¡Œé¦–è¡Œå°¾ç©ºç™½
    result = result
        .split('\n')
        .map(line => line.trim())
        .filter(line => line.length > 0)
        .join('\n');

    // æ¸…ç†ç©ºç™½ HTML tag / é›œè¨Š
    result = result.replace(/<[^>]*>/g, '');

    // æ¸…é™¤å¸¸è¦‹ç„¡æ„ç¾©çŸ­èªï¼ˆå¯æ ¹æ“šéœ€æ±‚å¢è£œï¼‰
    result = result.replace(/(é¦–æ³¢å·¥å•†æ™‚é–“|ç‰¹åˆ¥æ„Ÿè¬|è«‹è¦‹å½±ç‰‡|æ•¬è«‹æœŸå¾…|æ›´å¤šè³‡è¨Šè«‹è¦‹.*?)/g, '');

    return result.trim();
}

// summarizeContent å‡½å¼
export async function summarizeContent(title, content, abortSignal) {
    const config = getConfig(); // Get config to check directOutput for prompt generation
    const prompt = buildSummaryPrompt(title, content, config.directOutput); // Pass directOutput to buildSummaryPrompt
    S().lastSummaryPrompt = prompt; // Store the prompt in state
    
    // fetchAI will now return either a JSON string (for Ollama/LMStudio if !directOutput)
    // or plain text (if directOutput or for cloud APIs).
    const aiResponse = await fetchAI(prompt, abortSignal); 
    
    // stripThink should be safe for both JSON strings and plain text.
    // If it's a JSON string, stripThink should ideally not modify it if there are no "thinking..." prefixes.
    return stripThink(aiResponse); 
}

// buildQAPrompt å‡½å¼
export function buildQAPrompt(question, pageTitle, qaHistory, summaryKeyPoints, pageSourceText) {
    const cfg = getConfig();
    let contextString = `é—œæ–¼ç¶²é ã€Œ${pageTitle || 'æœªçŸ¥æ¨™é¡Œ'}ã€çš„å…§å®¹ã€‚\n`;

    if (summaryKeyPoints && summaryKeyPoints.length > 0) {
        contextString += "è©²ç¶²é çš„AIæ‘˜è¦é‡é»å¦‚ä¸‹ (é€™äº›æ‘˜è¦æ˜¯åŸºæ–¼ä¸‹æ–¹æä¾›çš„å®Œæ•´åŸå§‹å…§å®¹ç”¢ç”Ÿçš„)ï¼š\n";
        summaryKeyPoints.forEach(p => {
            const detailSnippet = p.details.length > 100 ? p.details.substring(0, 100) + "..." : p.details;
            contextString += `- ${p.title}: ${detailSnippet}\n`;
            if (p.quote) {
                const quoteSnippet = p.quote.length > 70 ? p.quote.substring(0, 70) + "..." : p.quote;
                contextString += `  (ç›¸é—œåŸæ–‡ç‰‡æ®µ: ${quoteSnippet})\n`;
            }
        });
        contextString += "\n";
    }

    if (pageSourceText && pageSourceText.length > 0) {
        contextString += `ä»¥ä¸‹æ˜¯è©²ç¶²é çš„å®Œæ•´åŸå§‹å…§å®¹ï¼Œè«‹å„ªå…ˆæ ¹æ“šæ­¤å…§å®¹ä¾†å›ç­”å•é¡Œï¼š\n--- åŸå§‹å…§å®¹é–‹å§‹ ---\n${pageSourceText}\n--- åŸå§‹å…§å®¹çµæŸ ---\n\n`;
    } else if (!summaryKeyPoints || summaryKeyPoints.length === 0) {
        contextString += `(æ³¨æ„ï¼šç›®å‰æ²’æœ‰æä¾›ç¶²é çš„è©³ç´°åŸå§‹å…§å®¹æˆ–AIæ‘˜è¦è³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚)\n\n`;
    }

    const relevantHistory = qaHistory.slice(0, -1).filter(h => h.a !== "â€¦").slice(-2);
    if (relevantHistory.length > 0) {
        contextString += "æˆ‘å€‘ä¹‹å‰çš„å•ç­”è¨˜éŒ„(æœ€è¿‘ç›¸é—œçš„2æ¢)ï¼š\n";
        relevantHistory.forEach(h => {
            const answerSnippet = typeof h.a === 'string' && h.a.length > 100 ? h.a.substring(0,100) + "..." : h.a;
            contextString += `ä½¿ç”¨è€…æå•ï¼š${h.q}\nå…ˆå‰å›ç­”ï¼š${answerSnippet}\n\n`;
        });
    }

    return `è«‹ä½ æ‰®æ¼”æœ¬æ–‡çš„ä½œè€…ï¼Œä¸¦ä»¥ç¬¬ä¸€äººç¨±è§’åº¦å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚ä½ å°å•é¡Œçš„å›ç­”å¿…é ˆ**å®Œå…¨æ ¹æ“šä¸‹æ–¹ã€Œä¸Šä¸‹æ–‡è³‡è¨Šã€ä¸­çš„å…§å®¹**ï¼Œä¸å¾—å¼•å…¥ä»»ä½•æœªè¢«æåŠçš„çŸ¥è­˜ã€æ¨æ¸¬æ€§è³‡æ–™ï¼Œæˆ–èˆ‡ä¸Šä¸‹æ–‡ç„¡é—œçš„å»ºè­°ã€‚

ä½†è«‹æ³¨æ„ï¼šç•¶ä½¿ç”¨è€…çš„å•é¡Œæ²’æœ‰è¢«æ˜ç¢ºæåŠæ™‚ï¼Œ**ä½ å¯ä»¥åœ¨ä¸Šä¸‹æ–‡å…§å®¹çš„åŸºç¤ä¸Šé€²è¡Œåˆä¹é‚è¼¯çš„æ¨è«–èˆ‡è©®é‡‹**ï¼Œå‰ææ˜¯é€™äº›æ¨è«–**æœ‰è·¡å¯å¾ª**ï¼Œä¸”**æ²’æœ‰é•èƒŒåŸæ„**ã€‚ä¾‹å¦‚ï¼šè‹¥æ–‡ç« æåˆ°ã€Œè¦æ ¼ä¸æ˜¯å¼·é …ï¼Œä½†è¨­è¨ˆä»¤äººé©šè‰·ã€ï¼Œä½ å¯ä»¥åˆç†ç†è§£ä½œè€…ä¸¦ä¸èªç‚ºè¦æ ¼å¾ˆå·®ï¼Œè€Œæ˜¯å°‡é‡é»æ”¾åœ¨è¨­è¨ˆä¸Šã€‚

è«‹éµå®ˆä»¥ä¸‹è¦ç¯„ï¼š
- åƒ…æ ¹æ“šä¸Šä¸‹æ–‡å…§å®¹ä½œç­”ï¼Œç¦æ­¢åŠ å…¥æœªè¢«æåŠçš„èƒŒæ™¯çŸ¥è­˜æˆ–å¤–éƒ¨è³‡è¨Š
- è‹¥ä¸Šä¸‹æ–‡ä¸­ç¢ºå¯¦æœªæåŠç›¸é—œå…§å®¹ï¼Œä¹Ÿç„¡æ³•é€²è¡Œåˆç†æ¨è«–ï¼Œè«‹ç›´æ¥è¡¨æ˜ç„¡æ³•å›ç­”ï¼Œä¾‹å¦‚ï¼šã€Œé€™éƒ¨åˆ†æˆ‘åœ¨æ–‡ç« ä¸­æ²’æœ‰æåŠã€æˆ–ã€Œæˆ‘æ²’æœ‰åœ¨å…§å®¹ä¸­èªªæ˜é€™å€‹å•é¡Œã€
- åƒ…ä½¿ç”¨ç´”æ–‡å­—ï¼Œè«‹å‹¿ä½¿ç”¨ä»»ä½• Markdown èªæ³•ï¼ˆå¦‚ï¼š#ã€*ã€- ç­‰ç¬¦è™Ÿï¼‰
- è‹¥å…§å®¹è¼ƒé•·ï¼Œå¯ä½¿ç”¨ç°¡å–®é»åˆ—ï¼Œæ¯åˆ—ä»¥ã€Œ- ã€é–‹é ­ï¼›è‹¥å…§å®¹ç°¡çŸ­ï¼Œè«‹ä»¥è‡ªç„¶æ®µè½æ–‡å­—å›ç­”

è«‹ä½¿ç”¨ã€Œ${cfg.outputLanguage}ã€ä½œç­”ã€‚

---ä¸Šä¸‹æ–‡è³‡è¨Šé–‹å§‹---
${contextString}
---ä¸Šä¸‹æ–‡è³‡è¨ŠçµæŸ---

ä½¿ç”¨è€…çš„å•é¡Œï¼š
${question}`;


}

// askAIQuestion å‡½å¼
export async function askAIQuestion(question, pageTitle, qaHistory, summaryKeyPoints, pageSourceText) {
    const prompt = buildQAPrompt(question, pageTitle, qaHistory, summaryKeyPoints, pageSourceText);
    // For Q&A, we generally expect plain text answers from fetchAI.
    // The new fetchAI logic should return plain text for QA prompts from all providers.
    const aiResponseText = await fetchAI(prompt, null); 
    return {
        answer: stripThink(aiResponseText), // stripThink on the plain text answer
        rawAnswer: aiResponseText, // Store the potentially stripped plain text
        prompt: prompt
    };
}

// getArticleContent å‡½å¼
export async function getArticleContent() {
    const cfg = getConfig();
    return new Promise((resolve, reject) => {
        if (!chrome.runtime?.id) {
            return reject(new Error(cfg.showErr ? "æ“´å……åŠŸèƒ½ç’°å¢ƒç„¡æ•ˆï¼Œç„¡æ³•ç™¼é€è¨Šæ¯" : "ç„¡æ³•å–å¾—å…§å®¹"));
        }
        chrome.runtime.sendMessage({ type: "EXTRACT_ARTICLE" }, response => {
            if (chrome.runtime.lastError) {
                return reject(new Error(cfg.showErr ? `ç„¡æ³•é€£æ¥åˆ°å…§å®¹è…³æœ¬: ${chrome.runtime.lastError.message}` : "ç„¡æ³•å–å¾—å…§å®¹ã€‚é–‹å•Ÿè¨­å®šä¸­çš„ã€Œé¡¯ç¤ºè©³ç´°éŒ¯èª¤è¨Šæ¯ã€ä»¥ç²å–æ›´å¤šè³‡è¨Šã€‚"));
            }
            if (response && typeof response.text === 'string' && typeof response.title === 'string') {
                resolve(response);
            } else if (response && response.error) {
                reject(new Error(cfg.showErr ? `æå–å…§å®¹éŒ¯èª¤: ${response.error}`: "ç„¡æ³•å–å¾—å…§å®¹ã€‚é–‹å•Ÿè¨­å®šä¸­çš„ã€Œé¡¯ç¤ºè©³ç´°éŒ¯èª¤è¨Šæ¯ã€ä»¥ç²å–æ›´å¤šè³‡è¨Šã€‚"));
            }
            else {
                reject(new Error(cfg.showErr ? "å…§å®¹è…³æœ¬å›å‚³çš„è³‡æ–™æ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©º" : "ç„¡æ³•å–å¾—å…§å®¹ã€‚é–‹å•Ÿè¨­å®šä¸­çš„ã€Œé¡¯ç¤ºè©³ç´°éŒ¯èª¤è¨Šæ¯ã€ä»¥ç²å–æ›´å¤šè³‡è¨Šã€‚"));
            }
        });
    });
}